{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "zsh:1: command not found: wget\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kailinx/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/kailinx/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "Train & Test Files are loaded\n"
     ]
    }
   ],
   "source": [
    "#@title Run this to load the datasets and setup the environment. { display-mode: \"form\" }\n",
    "# Run this every time you open the spreadsheet\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import Counter\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torchtext.vocab import GloVe\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords' ,quiet=True)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn import metrics\n",
    "import gdown\n",
    "import string\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pad_sequences\n",
    "import string\n",
    "\n",
    "\n",
    "def get_finance_train():\n",
    "  df_train = pd.read_csv(\"finance_train.csv\")\n",
    "  return df_train\n",
    "def get_finance_test():\n",
    "  df_test = pd.read_csv(\"finance_test.csv\")\n",
    "  return df_test\n",
    "\n",
    "PUNCTUATION = '!#$%&()*,-./:;<=>?@^_`{|}~'\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwords from text\n",
    "    return text\n",
    "\n",
    "def pad_sequences_train(df_train, df_test):\n",
    "  tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "  tokenizer.fit_on_texts(df_train['Sentence'].values)\n",
    "  word_index = tokenizer.word_index\n",
    "  X = tokenizer.texts_to_sequences(df_train['Sentence'].values)\n",
    "  X_train = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "  return X_train\n",
    "\n",
    "def pad_sequences_test(df_train, df_test):\n",
    "  tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "  tokenizer.fit_on_texts(df_train['Sentence'].values)\n",
    "  word_index = tokenizer.word_index\n",
    "  X = tokenizer.texts_to_sequences(df_test['Sentence'].values)\n",
    "  X_test = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "  return X_test\n",
    "\n",
    "def run_model(X_train, y_train, X_test, y_test, epochs=5, max_sequence_length=256, max_nb_words=1000, embedding_dim=300):\n",
    "  if any(x is None for x in [X_train, y_train, X_test, y_test, epochs, max_sequence_length, max_nb_words, embedding_dim]):\n",
    "    print('Replace the None values above with your new values before calling the run_model() function.')\n",
    "    return None, None, None\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(max_nb_words+1, embedding_dim, mask_zero=True, input_length=max_sequence_length))\n",
    "  model.add(SpatialDropout1D(0.2))\n",
    "  model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "  model.add(Dense(n_labels, activation='softmax'))\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  print(model.summary())\n",
    "  history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "  test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "  return model, history, test_accuracy\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_test.csv'\n",
    "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_train.csv'\n",
    "\n",
    "print (\"Train & Test Files are loaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
